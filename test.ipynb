{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Extract Text from PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "file_name = 'dataset/divai2020_benkova.pdf'\n",
    "pdf_file_text = extract_text(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/344476131\n",
      "\n",
      "Neural Machine Translation as a Novel Approach to Machine Translation\n",
      "\n",
      "Conference Paper · September 2020\n",
      "\n",
      "CITATIONS\n",
      "3\n",
      "\n",
      "2 authors:\n",
      "\n",
      "READS\n",
      "2,353\n",
      "\n",
      "Lucia Benková\n",
      "\n",
      "Ľubomír Benko\n",
      "\n",
      "University of Constantinus the Philosopher in Nitra - Univerzita Konstant’na Filoz…\n",
      "\n",
      "University of Constantinus the Philosopher in Nitra - Univerzita Konstant’na Filoz…\n",
      "\n",
      "9 PUBLICATIONS   10 CITATIONS   \n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "44 PUBLICATIONS   193 CITATIONS   \n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "Some of the authors of this publication are also working on these related projects:\n",
      "\n",
      "Abbreviation dictionary View project\n",
      "\n",
      "Adaptation of the learning process using sensor networks and the Internet of Things View project\n",
      "\n",
      "All content following this page was uploaded by Ľubomír Benko on 05 October 2020.\n",
      "\n",
      "The user has requested enhancement of the downloaded file.\n",
      "\n",
      "\fNeural\tMachine\tTranslation\tas\ta\tNovel\tApproach\tto\t\n",
      "Machine\tTranslation\t\n",
      "\n",
      "Lucia Benkov\n",
      "\n",
      "Department of Informatics, Constantine the Philosopher University in Nitra, Nitra, Slovakia  \n",
      "lucia.benkova@ukf.sk, lbenko@ukf.sk \n",
      "\n",
      "Abstract \n",
      "\n",
      "The  aim  of  the  paper  is  to  present  the  most  used  machine  translation-  Statistical  Machine \n",
      "Translation system and introduce a novel system- Neural Machine Translation. Neural Machine \n",
      "Translation  structure  is  built  on  an  encoder-decoder  framework.  The  encoder  transforms  a \n",
      "source  language  sentence  into  continuous  space  representation  through  a  recurrent  neural \n",
      "network. Origin of neural networks was inspired by the understanding of the functioning of the \n",
      "human brain, or all connections between neurons. However, in contrast with the human brain, \n",
      "where  neurons  can  freely  interconnect,  artificial  neural  networks  consist  of  discrete  layers, \n",
      "connections, and data dissemination. This paper deals with neural machine translation as a novel \n",
      "approach  that  is  examined  by  many  researchers  that  try  to  implement  it  into  already  used \n",
      "frameworks. The results  show that neural machine translation offers an improvement of the \n",
      "translation output but still has to be evaluated in the future. \n",
      "\n",
      "Keywords \n",
      "\n",
      "Natural  Language  Processing.  Machine  Translation  System.  Neural  Machine  Translation. \n",
      "Statistic Machine Translation. \n",
      "\n",
      "INTRODUCTION\t\n",
      "\n",
      "In  the  beginning,  machine  translation  was  based  mainly  on  Rule-based  Machine \n",
      "Translation (RBMT), the idea being to create grammatical rules for the source and target \n",
      "language. Machine translation acted as a kind of translation between languages based on \n",
      "this  set  of  rules.  However,  the  problem  was  mainly  the  addition  of  new  content,  new \n",
      "language  pairs,  because  maintaining  and  extending  such  a  set  of  rules  was  too  time-\n",
      "consuming and costly. Statistical Machine Translation (SMT) was created to overcome this \n",
      "problem (Koehn, 2010). SMT systems create statistical models by analyzing an aligned set \n",
      "of source and target language sentences (training set). It is then used to create a translation. \n",
      "The advantage of SMT is its automatic learning process and relatively easy adaptation. The \n",
      "disadvantage of SMT is the training itself, so it is necessary to create a usable tool and a \n",
      "large database of source and target language segments. The disadvantage is also difficult to \n",
      "work with grammatically more complicated languages. Neural Machine Translation (NMT) \n",
      "has recently started to be promoted for this reason. NMT looks at the sentence as a whole \n",
      "and can form associations between phrases even at greater distances in the sentence. The \n",
      "result should be improved by grammatical accuracy compared to SMT. \n",
      "\n",
      "DIVAI 2020 \n",
      "ISBN 978-80-7598-841-6 ISSN 2464-7470 (Print) ISSN 2464-7489 (On-line) \n",
      "\n",
      " The 13th international scientific conference on Distance Learning in Applied Informatics. \n",
      "\n",
      "499 \n",
      "\n",
      " \n",
      " \n",
      "\fNeural Machine Translation as a Novel Approach to Machine Translation \n",
      "\n",
      "SMT and NMT operate on a statistical basis and use pairs of source and target language \n",
      "segments  as  a  basis.  In  principle,  SMT  is  Phrase-Based  Statistical  Machine  Translation \n",
      "(PBSMT), which means that SMT divides source segments into phrases (Koehn, 2010). SMT \n",
      "creates  a  translation  and  language  model  during  training.  The  translation  model  stores \n",
      "various phrase translations, and the language model stores the likelihood of a sequence of \n",
      "phrases on the target page. During the translation, the decoder selects a translation that \n",
      "works best based on these two models. In principle, SMT can produce very good results at \n",
      "the  level  of  phrases,  but  the  fluency  and  grammar  of  the  translation  are  lagging  behind \n",
      "several times. The paper describes the novel approach of neural machine translation and its \n",
      "usage by other researchers.  \n",
      "\n",
      "The rest of the paper is structured as follows. The next section describes the research \n",
      "background and introduces the main topics of Statistical Machine  Translation and Neural \n",
      "Machine  Translation.  The  third  section  summarizes  the  usage  of  the  novel  approach  to \n",
      "Machine Translation. The last section provides the conclusion. \n",
      "\n",
      "RESEARCH\tBACKGROUND\t\n",
      "\n",
      "Statistical  machine  learning  (SMT)  is  an  approach  to  Machine  Translation  that  is \n",
      "characterized  by  the  usage  of  machine  learning  methods.  SMT  treats  translation  as  a \n",
      "machine learning problem (Lopez, 2008). The basis of SMT is to create a system that can \n",
      "automatically  discover  translation  rules  of  the  large  bilingual  corpus,  merge  starting \n",
      "\n",
      "results  of  the  statistical  analysis  of  relevant  data  (Koehn,  2010).  Statistical  machine \n",
      "translation  deals  with  the  translation  of  text  from  one  natural  language  to  another.  Its \n",
      "approach to machine translation is characterized by the usage of machine learning methods. \n",
      "This  means  that  the  learning  algorithm  is  applied  to  a  large  group  of  the  previously \n",
      "translated  text,  referred  to  as  parallel  corpus,  parallel  text,  bitext  or  multi-text.  This \n",
      "approach  uses  the  power  of  computers  to  create  sophisticated  data  models  capable  of \n",
      "translating  text  from  one  language  to  another.  Basically,  statistical  machine  translation \n",
      "systems use computer algorithms to create a translation that selects the best and most likely \n",
      "statistically output of the millions of possible permutations. \n",
      "\n",
      "The  advantage  of  statistical  machine  translation  systems  is  the  removal  of  manual \n",
      "translator  work  for  each  language  pair.  On  the  other  hand,  the  disadvantage  is  the \n",
      "restriction to a single region of texts (domain), i.e. if the system is trained on one type of \n",
      "corpus (e.g. administrative), then it should be used to translate administrative texts, not e.g. \n",
      "technical  texts.  The  quality  of  the  translation  would  be  unpublished  in  this  case,  and \n",
      "therefore it is important to train the system with the corpus, which is thematically similar \n",
      ".  As  statistical  machine  translation  has \n",
      "to  the  starting  text \n",
      "evolved over the years, its systems have evolved and improved too. In the very beginning, \n",
      "separate word translation was used, but progress in machine translation and in science itself \n",
      "was mainly in rapid development. New systems, larger collections of parallel corpora, and \n",
      "more  powerful  computers  have  continually  improved  the  quality  of  statistical  machine \n",
      "translation. \n",
      "\n",
      "DIVAI 2020 \n",
      "ISBN 978-80-7598-841-6 ISSN 2464-7470 (Print) ISSN 2464-7489 (On-line) \n",
      "\n",
      " The 13th international scientific conference on Distance Learning in Applied Informatics. \n",
      "\n",
      "500 \n",
      "\n",
      " \n",
      " \n",
      "\fNeural Machine Translation as a Novel Approach to Machine Translation \n",
      "\n",
      "The first systems for statistical machine translation were based on the translation of \n",
      "individual words. Although this system is no longer widely used, many of its principles and \n",
      "methods are still up to date.  \n",
      "\n",
      "The smallest units in this system are words that can be translated, inserted, omitted, or \n",
      "their order in the sentence changed. These systems are based solely on lexical translation - \n",
      "the translation of isolated words. It requires dictionaries that map the translation of words \n",
      "from one language to another (Koehn, 2010). Looking at a common vocabulary, we find that \n",
      "a word can have more meanings in a foreign language. Some of them are used more, some \n",
      "less. As an example, a translation of the German word Haus into English can be used. In this \n",
      "case,  the  English  word  house  will  in  most  cases  be  considered  the  correct  translation. \n",
      "Options such as building or home are also common, while others are used only in certain \n",
      "specific circumstances, e.g. the word shell that can refer to the slug home. \n",
      "\n",
      "The correct translation or the most probable possibility of translation is then selected \n",
      "using parallel corpora. Let's say that in the hypothetical text the word Haus would appear \n",
      "10 000 times. Of which 8 000 would be translated as house, 1 600 times as building, 200 \n",
      "times as home, etc. Based on these calculations, it can be estimated the likelihood of a lexical \n",
      "translation. Formally speaking, the aim is to find a function \n",
      "\n",
      "which will help in translating another German text to determine what translation of Haus is \n",
      "most likely. This function returns the foreign word f (in this case the word Haus) and the \n",
      "probability for each of the possible translations e. This will tell how likely it is to have the \n",
      "correct translation. \n",
      "\n",
      "A machine translation system based on the translation of individual words was already \n",
      "mentioned.  Words  like  the  smallest  translation  units,  may  not  be  the  best  choice. \n",
      "Sometimes one word in a foreign language is translated into two English words or vice versa. \n",
      "Word-based  models  often  diverge  and  differ  in  these  cases.  In  more  advanced statistical \n",
      "machine  translation,  the  basic  unit  of  translation  is  expanded  from  words  to  phrases  of \n",
      "potentially unlimited length and may not be defined as phrases from a syntactic point of \n",
      "view (Chiang, 2007). \n",
      "\n",
      "At present, one of the best systems for statistical machine translation is considered on \n",
      "phrase-based models - systems that translate a small sequence of words at once. In phrase \n",
      "models, any sequence of contiguous words can be considered a phrase. Each input phrase \n",
      "is non-empty and is translated exactly to one non-empty output phrase. However, phrases \n",
      "are not required to have the same length, so this model can produce translations of varying \n",
      "length (Lopez, 2008). \n",
      "\n",
      "Phrase translation systems work by dividing the input sentence into segments - phrases \n",
      "(polyword  units).  Each  of  these  segments  is  translated  into  the  target  language  and  the \n",
      "phrases  are  finally  sorted.  However,  the  number  of  phrases  at  the  input  and  language \n",
      "targets may not match. \n",
      "\n",
      "One of the basic elements in any statistical machine translation is a language model \n",
      "that measures the likelihood of a given word sequence that will be actually used by English-\n",
      "speaking person. It goes without saying that it is required of the machine translation system \n",
      "not only to produce output words that are correct with respect to the original text but also \n",
      "to put them in the right string (Koehn, 2010). The language model, however, usually does \n",
      "\n",
      "DIVAI 2020 \n",
      "ISBN 978-80-7598-841-6 ISSN 2464-7470 (Print) ISSN 2464-7489 (On-line) \n",
      "\n",
      " The 13th international scientific conference on Distance Learning in Applied Informatics. \n",
      "\n",
      "501 \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\fNeural Machine Translation as a Novel Approach to Machine Translation \n",
      "\n",
      "much more than just allows smooth output. It supports difficult decisions on word order \n",
      "and word translation. For example, the probabilistic language model \n",
      " should prefer the \n",
      "correct word order instead of the wrong word order: \n",
      "\n",
      ". \n",
      "\n",
      "Formally speaking, a language model is a function that takes an English sentence and \n",
      "returns the probability that the sentence was created by an English-speaking person. Based \n",
      "on the example above, it is more likely that an English-speaking person would rather say a \n",
      "sentence the house is small than small the is house. Therefore, a good language model of \n",
      "\n",
      " assigns a higher probability of the first sentence. \n",
      "\n",
      "This advantage of the language model helps statistical machine translation systems to \n",
      "find the right word order. Another area where the language model helps translation is the \n",
      "choice  of  words.  If  a  foreign  word  (for  example,  German  Haus)  has  several  translations \n",
      "(house, home, ...), the more common translation (in this case house) will be favoured based \n",
      "on the likelihood of a lexical translation. However, other translations may be appropriate in \n",
      "certain specific contexts. Here the language model that gives a higher likelihood of a more \n",
      "natural choice of words in the context, is applied again. For example: \n",
      "\n",
      ". \n",
      "\n",
      "One of the main methods in the language model is the N-gram language model. N-gram \n",
      "is  a  term  commonly  used  with  language  models  for  speech  recognition.  It  can  give  the \n",
      "probability of the next word, based on the previous sequence of words from the training \n",
      "corpus (Yamamoto et al., 2003). The principle of modelling language using n-grams is that \n",
      "the model divides the sentence into several fragments (words/phrases) that often occur in \n",
      "the  corpus  and  carry  language  information  and  determine  the  probability  of  individual \n",
      "fragments.  If  the  fragments  of  the  sentence  are  in  the  correct  order,  then  the  sentence \n",
      "should have a high probability \n",
      ". Returning to the example, after \n",
      "analyzing a great deal of text, it was identified that going is followed by home more often \n",
      "than house. \n",
      "\n",
      "Formally speaking, in language models, it is anticipated to calculate the probability of a \n",
      "\n",
      "string: \n",
      "\n",
      ". \n",
      "\n",
      "Simplistically, \n",
      "\n",
      "  is  the  probability  of  randomly  selecting  a  sequence  of  English \n",
      ", it is needed to \n",
      "words (whether in a book or a magazine) and getting \n",
      "collect a large amount of text, where is calculated how often \n",
      " is present. Most of the long \n",
      "word sequences, however, will not be found in the text at all. Therefore, it is required to \n",
      "analyze  the  calculation  of \n",
      "  into  smaller  steps  for  which  can  be  collected  sufficient \n",
      "statistics and further divide the probability estimate. \n",
      "\n",
      ". To calculate \n",
      "\n",
      "Dealing with the limited amount of data that limits us in gathering enough statistics to \n",
      "\n",
      "reliably estimate the probability of distribution is a major problem in language models. \n",
      "\n",
      "On  the  other  hand,  the  Neural  Machine  Translation  (NMT)  structure  is  built  on  an \n",
      "encoder-decoder  framework.  The  encoder  transforms  a  source  language  sentence  into \n",
      "continuous space representation through a recurrent neural network (RNN) from which the \n",
      "decoder generates a target language sentence using another RNN (Cheng, 2019). NMT uses \n",
      "deep learning, which in principle is represented by the neural network  (Bessenyei, 2017; \n",
      "\n",
      "DIVAI 2020 \n",
      "ISBN 978-80-7598-841-6 ISSN 2464-7470 (Print) ISSN 2464-7489 (On-line) \n",
      "\n",
      " The 13th international scientific conference on Distance Learning in Applied Informatics. \n",
      "\n",
      "502 \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fNeural Machine Translation as a Novel Approach to Machine Translation \n",
      "\n",
      "Stencl and Stastny, 2010). Machine learning could simply be understood by algorithms that \n",
      "process  data  from  which  they  can  learn,  and  on  that  basis,  they  can  make  decisions  or \n",
      "predict  solutions  to  certain  problems.  Origin  of  neural  networks  was  inspired  by  the \n",
      "understanding of the functioning of the human brain, or all connections between neurons. \n",
      "However, in contrast with the human brain, where neurons can freely interconnect, artificial \n",
      "neural  networks  consist  of  discrete  layers,  connections,  and  data  dissemination.  Neural \n",
      "information  processing  to  perform  calculations. \n",
      "networks  use  distributed,  parallel \n",
      "Knowledge is stored primarily through the strength of links between individual neurons. The \n",
      "basic  feature  of  neural  networks  is  learning.  The  neuron  receives  signals  from  the \n",
      "environment from other neurons, processes them and sends them as input signals for the \n",
      "neurons in its surroundings. Multilayer neural networks consist of three layers (Fig. 1): \n",
      "\n",
      "1.  Input layer - the input is from the external world and the output is another neuron, \n",
      "\n",
      "2.  Hidden layer - input is from the external world or from other neurons, the output is \n",
      "\n",
      "another neuron, \n",
      "\n",
      "3.  Output layer - the input is similar to the hidden layer and the output is directed to \n",
      "\n",
      "the external world. \n",
      "\n",
      "Figure1 Multilayer neural networks (Cheng, 2019) \n",
      "\n",
      ". In other words,  and \n",
      "\n",
      "Each  neuron  assigns  some  weight  to  its  input.  The  weight  represents  the  degree  of \n",
      "fulfilment of the task being performed, the higher weight means the better solution. The \n",
      "final  output  of  the  neural  network  is  thus  affected  by  the  total  sum  of  the  weights. \n",
      " and \n",
      "The essence of machine translation is the different length of input \n",
      "output \n",
      " may not be the same. For this reason, \n",
      "it is necessary to use a special type of neural networks - recurrent neural network. The RNN \n",
      "retains its internal state as long as it reads the sequence of inputs, in this case, a sequence \n",
      "of words, and can process the input of different lengths. The goal of RNN is to compact the \n",
      "sequence of input symbols into a fixed vector by recursion. Recursion in simplicity means \n",
      "defining a function or method by itself. The overall architecture is based on the encoder-\n",
      "decoder principle (Kalchbrenner a Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., \n",
      "2016; Cho et al., 2014).  \n",
      "\n",
      "The encoder is a straightforward RNN application based on sequential summaries, i.e. \n",
      "an activation function is recursively applied to an input sequence or sentence until the last \n",
      "input state of RNN is a summary of the entire input sentence. First, each word of the source \n",
      "sentence is represented as a so-called 1-of-K encoded vector. Words are equidistant from \n",
      "each  other,  which  means  that  there  is  no  relationship  between  words.  A  hierarchical \n",
      "approach is used to extract a sentence representation (a vector that summarizes an input \n",
      "sentence). In principle, the network will learn from data. The encoder then linearly projects \n",
      "\n",
      "DIVAI 2020 \n",
      "ISBN 978-80-7598-841-6 ISSN 2464-7470 (Print) ISSN 2464-7489 (On-line) \n",
      "\n",
      " The 13th international scientific conference on Distance Learning in Applied Informatics. \n",
      "\n",
      "503 \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\fNeural Machine Translation as a Novel Approach to Machine Translation \n",
      "\n",
      "the 1-of-K encoded vector using an  matrix that has as many columns as there are words \n",
      "in the source dictionary and as many lines as the programmer chooses (typically 100-500). \n",
      "The  projection  results  in  a  continuous  vector  for  each  source  word.  Each  vector  is  later \n",
      "updated to improve compiler performance. \n",
      "\n",
      "When  a  fixed  sentence  representation  of  a  source  sentence  is  created  using  the \n",
      "encoder and RNN, we use the decoder with RNN to create a translation. Starting from RNN, \n",
      "the internal state of RNN is calculated based on the source sentence summation vector, the \n",
      "preceding word, and the previous internal state. Using the internal hidden state it is possible \n",
      "to score each target word based on how likely it will follow all previously translated words \n",
      "based on the source sentence. This is possible by assigning probabilities to each word. The \n",
      "difference between score and probability is that the sum of the probabilities of all possible \n",
      "words equals 1, but the score does not need to be 1. Based on the score, the next step is to \n",
      "calculate  the  probability  that  serves  to  select  a  word  by  choosing  from  a  multinomial \n",
      "distribution. After selecting the i-th word, it returns to the first step, calculating the hidden \n",
      "state of the decoder, evaluating and normalizing the target word, and selecting the next \n",
      "  word.  The  procedure  is  repeated  until  the  end  of  the  sentence  (called  <eos>)  is \n",
      "reached. By using a neural network, translation performance can be maximized (Bahdanau \n",
      "et al., 2016). \n",
      "\n",
      "Corpus is needed to train a neural network and the maximum log-likelihood estimation \n",
      "method is used (Cheng, 2019). Each corpus element is a pair of source and target sentences. \n",
      "Each  sentence  is  a  sequence  of  numerical  indexes  corresponding  to  words,  which  is \n",
      "equivalent to binary vectors (one element vector is set to 1). During the training process, \n",
      "the  NMT  system  attempts  to  set  the  neural  network  weights  parameter  based  on  the \n",
      "reference  values  (translation  from  target  to  source  language).  Taking  any  pair  from  the \n",
      "corpus, the NMT system can calculate the conditional log-probability of the target sentence \n",
      "from the source sentence. The result is a neural network that can process source segments \n",
      "and transform them into target segments, with NMT passing through whole sentences, not \n",
      "just  phrases.  The  advantage  of  this  approach  is  precisely  the  appropriate  context  of  the \n",
      "translation,  which  also  improves  the  fluency  of  the  translation.  But  the  accuracy  of  the \n",
      "terminology can sometimes be insufficient. \n",
      "\n",
      "MACHINE\tTRANSLATION\tAPPLICATIONS\t\n",
      "\n",
      "SMT is used for many years to produce the output for various language pairs. Authors \n",
      "in \n",
      " focused on the preparation of text where it depends on the \n",
      "data sources used. The aim of this work was to determine to what extent it is necessary to \n",
      "carry out the time-consuming data pre-processing in the process of discovering sequential \n",
      " focused on the evaluation \n",
      "patterns in e-documents. Munkova et al. \n",
      "of  translation  quality  of  sentences  of  the  MT  output  and  post-edited  MT  output.  The \n",
      "authors' used metrics of automatic MT evaluation for a language pair Slovak-German. The \n",
      "MT translation was done using an SMT system. Munk and Munkova (Munk and Munkova, \n",
      "2018; Munkova and Munk, 2015) introduced an exploratory data technique representing an \n",
      "instrument  to  evaluate  and  improve  MT  systems.  The  authors  used  residual  analysis  to \n",
      "identify  the  differences  between  an  SMT  system  output  and  post-edited  MT  regarding \n",
      "human translation. Using residual analysis, the authors identified sentences that contained \n",
      "significant differences for the scores of automatic  metrics between MT output and post-\n",
      "\n",
      "DIVAI 2020 \n",
      "ISBN 978-80-7598-841-6 ISSN 2464-7470 (Print) ISSN 2464-7489 (On-line) \n",
      "\n",
      " The 13th international scientific conference on Distance Learning in Applied Informatics. \n",
      "\n",
      "504 \n",
      "\n",
      " \n",
      " \n",
      "\fNeural Machine Translation as a Novel Approach to Machine Translation \n",
      "\n",
      "edited MT output from Slovak into English. A system for post-editing the SMT system output \n",
      "was presented by \n",
      ". The aim of the study (Munkova et al., 2019) is to \n",
      "compare translation quality and effectiveness (translator productivity) using measures of \n",
      "the automatic evaluation of machine translation output. The examined translation(s) was a \n",
      "legal  text,  translated  from  Slovak  (mother  tongue)  into  German.  We  distinguish  human \n",
      "translation (HT), machine translation (MT) and post-edited MT (PEMT). For the evaluation \n",
      "we used our own tool, wherein were implemented the metrics of automatic MT evaluation. \n",
      "\n",
      "Many  authors  analyze  neural  machine  translation  as  a  novel  approach  and  try  to \n",
      "implement it into already used frameworks. Banik et al. (Banik et al., 2019) analyzed the a \n",
      "statistical  approach  to  combine  the  outputs of  various  machine  translation  systems.  The \n",
      "\n",
      "merged them into the final translation. The used NMT systems were Google Translate (Wu \n",
      "et  al.,  2016)  and  Bing  Microsoft  Translate  (Dolan  et  al.,  2002)  and  the  authors  used  a \n",
      "Hierarchical  system.  The  experiment  was  done  using  8  different  language  pairs  and  the \n",
      "results were evaluated based on a fuzzy-based MT evaluation metric LeBleu (Virpioja and \n",
      ". The results of the experiment showed that the outputs obtained by Google \n",
      "and  Bing  are very  similar  most  of  the  times.  The  output  from  the  SMT  system  may  have \n",
      "different word orders or synonyms. The system combination model produces translations \n",
      "matching with those of Google and Bing Microsoft Translate. Bentivogli et al. (Bentivogli et \n",
      "al., 2018) compared the NMT with the phrase-based MT system on an English-German and \n",
      "English-French  dataset.  The  analysis  was  done  thoughtfully.  Not  only  did  the  authors \n",
      "evaluate the translation quality using TER and HTER metrics but also based on morphological \n",
      "analysis.  The  morphological  analysis  consisted  of  identifying  lexical  errors,  morphology \n",
      "errors and word order errors. The lexical analysis has shown various results. The NMT results \n",
      "for proper nouns were worse than for the phrase-based MT in the case of English-French \n",
      "language pair. On the other hand, the NMT showed better handling of complex sentences \n",
      "in the case of English-German language pair. The morphological error identification showed \n",
      "that  NMT  makes  considerably  less  morphology  errors  in  both  language  pairs.  The  word \n",
      "ordering errors analysis showed that this issue is language specific. The NMT has done well \n",
      "for  the  English-German  language  pair  where  it was  successful  at  generating  well-formed \n",
      "sentences. Also in the case of language pair with less complex reordering phenomena the \n",
      "NMT  performed  better  than  phrase-based  MT.  Bentivogli  et  al.  (Bentivogli  et  al.,  2018) \n",
      "showed that NMT is superior to phrase-based MT but identified also some shortcomings of \n",
      "NMT.  NMT  has  issues  with  the  translation  of  proper  nouns  and  with  the  reordering  of \n",
      "particular linguistic constituents. Xia (Xia, 2019) introduced a statistical machine translation \n",
      "system  based  on  deep  neural  network.  The  focus  of  the  article  is  oriented  to  the  word \n",
      "alignment  and  pre-ordering  in  SMT.  The  word  alignment  model  was  created  by  a \n",
      "combination  of  multi-layer  neural  network  and  undirected  probability  graph  model.  The \n",
      "linearly ordered pre-ordering model was created using the multi-layer neural network to \n",
      "vocabulary  the  representation.  Both  of  these  models  were  combined  in  the  same  deep \n",
      "neural  network  framework  named DNNAPM.  The  framework  was  tested  on  a  sample  of \n",
      "100 000 sentence pairs. Accuracy was used as the evaluation metric for the field of word \n",
      "segmentation. Marzouk and Hansen-Schirra (Marzouk and Hansen-Schirra, 2019) analyzed \n",
      "the  application  of  controlled  language  to  improve  the  machine  translation  output.  The \n",
      "authors compared the impact of nine controlled language rules to the quality of NMT output \n",
      "and compared the results for other MT systems: rule-based, statistical and hybrid MT. The \n",
      "experiment was done with the English-German language pair using texts of the technical \n",
      "\n",
      "DIVAI 2020 \n",
      "ISBN 978-80-7598-841-6 ISSN 2464-7470 (Print) ISSN 2464-7489 (On-line) \n",
      "\n",
      " The 13th international scientific conference on Distance Learning in Applied Informatics. \n",
      "\n",
      "505 \n",
      "\n",
      " \n",
      " \n",
      "\fNeural Machine Translation as a Novel Approach to Machine Translation \n",
      "\n",
      "enko \n",
      "\n",
      "domain.  The  results  of  the  experiment  showed  that  NMT  behaves  differently  when \n",
      "controlled  language  is  applied.  The  quality  of  the  NMT  output  is  higher  without  the \n",
      "application  of  controlled  language.  This  is  in  contradiction  with  other  MT  (rule-based, \n",
      "statistical, and hybrid) where the application of controlled language improves the output of \n",
      "the translation. The NMT system obtained the best results between all of the MT systems \n",
      "regardless of the application of the controlled language. The limitation of the experiment \n",
      "was  in  the  use  of  only  one  language  pair  and  the  experiment  should  be  done  for  other \n",
      "language  pairs.  The  application  of  the  controlled  language  did  not  bring  any  expected \n",
      "results. Based on the results of the experiment it seems that the application of controlled \n",
      "language could become obsolete for the novel MT system. Pinnis et al. (Pinnis et al., 2018b) \n",
      "presented an integration of NMT systems into document workflow translation of a cloud-\n",
      "based translation system and introduced examples of formatting-rich document translation. \n",
      "Pinnis et al. (Pinnis et al., 2018a) validated the NMT application to more difficult language \n",
      "pairs with less resources available for the NMT training. The authors compared the SMT and \n",
      "NMT  systems  for  highly  inflected  languages  (Estonian,  Latvian  and  Russian).  The  authors \n",
      "also compared the results of the SMT and NMT systems output for a broad data domain and \n",
      "narrow  data  domain.  The  MT  output  was  evaluated  using  automated  (BLEU,  NIST,  and \n",
      "ChrF2)  and  manual  methods  (system  comparative  evaluation  and  error  analysis  of \n",
      "translations). The results of the evaluation showed that NMT system achieved better results \n",
      "for 83 % language pairs of broad domain. On the other hand, the narrow domain results \n",
      "showed that the SMT system produced significantly better translations than NMT system.  \n",
      "\n",
      "CONCLUSION\t\t\n",
      "\n",
      "In this paper was described the novel approach of machine translation- neural machine \n",
      "translation system and the most used statistical  machine translation system. Both  of the \n",
      "machine translation systems were introduced and described in detail. The SMT as the most \n",
      "used system is getting replaced by the NMT as a novel approach. This paper presented the \n",
      "use of NMT by other researchers. Other authors start to use it and analyze its potential for \n",
      "the  specific  language  pairs.  The  results  of  the  described  experiments  show  a  potential \n",
      "improvement of the translation output of NMT in comparison with the SMT output. Despite \n",
      "that,  there  are  some  shortcomings  of  the  novel  NMT  system  where  the  SMT  still  offers \n",
      "better  results.  The  future  work  would  be  focused  on  a  detailed  analysis  of  the  Neural \n",
      "Machine Translation system output for a flective language such as the Slovak language. The \n",
      "research would compare the difference of output of SMT and NMT systems. Also, it would \n",
      "be interesting to compare Google Translate before it changed to NMT and observe whether \n",
      "the change improved the translation quality also for flective languages. \n",
      "\n",
      "ACKNOWLEDGEMENT\t\n",
      "\n",
      "This work was supported by the Slovak Research and Development Agency under the \n",
      "contract No. APVV-18-0473 and Scientific Grant Agency of the Ministry of Education of the \n",
      "Slovak Republic (ME SR) and of Slovak Academy of Sciences (SAS) under the contracts No. \n",
      "VEGA-1/0809/18. \n",
      "\n",
      "DIVAI 2020 \n",
      "ISBN 978-80-7598-841-6 ISSN 2464-7470 (Print) ISSN 2464-7489 (On-line) \n",
      "\n",
      " The 13th international scientific conference on Distance Learning in Applied Informatics. \n",
      "\n",
      "506 \n",
      "\n",
      " \n",
      "\fNeural Machine Translation as a Novel Approach to Machine Translation \n",
      "\n",
      "REFERENCES\t\n",
      "\n",
      "Bahdanau, D., Cho, K., Bengio, Y., 2016. Neural machine translation by jointly learning to align \n",
      "\n",
      "and translate. Proc. Int. Conf. Learn. Represent. 15. \n",
      "\n",
      "Banik,  D.,  Ekbal,  A.,  Bhattacharyya,  P.,  Bhattacharyya,  S.,  Platos,  J.,  2019.  Statistical-based \n",
      "system  combination  approach  to  gain  advantages  over  different  machine  translation \n",
      "systems. Heliyon 5, e02504. \n",
      "\n",
      "Bentivogli,  L.,  Bisazza,  A.,  Cettolo,  M.,  Federico,  M.,  2018.  Neural  versus  phrase-based  MT \n",
      "quality: An in-depth analysis on English German and English French. Comput. Speech Lang. \n",
      "49, 52 70. \n",
      "\n",
      "Bessenyei,  G.,  2017.  Neural  Machine  Translation:  The  Rising  Star  [WWW  Document]. \n",
      "https://www.memsource.com/blog/2017/09/19/neural-machine-\n",
      "\n",
      "Memsource. \n",
      "URL \n",
      "translation-the-rising-\n",
      "star/?utm_source=mailchimp&utm_medium=email&utm_content=blog_article \n",
      "10.4.17). \n",
      "\n",
      "(accessed \n",
      "\n",
      "Cheng, Y., 2019. Neural Machine Translation. In: Joint Training for Neural Machine Translation. \n",
      "\n",
      "Springer, Singapore, pp. 1 10. \n",
      "\n",
      "Chiang, D., 2007. Hierarchical phrase-based translation. Comput. Linguist. 33, 201 228. \n",
      "\n",
      "Cho,  K.,  Bahdanau,  D.,  Bougares,  F.,  Schwenk,  H.,  Bengio,  Y.,  2014.  Learning  Phrase \n",
      "Representations  using  RNN  Encoder-Decoder  for  Statistical  Machine  Translation.  In: \n",
      "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing \n",
      "(EMNLP). Association for Computational Linguistics, Doha, Qatar, pp. 1724 1734. \n",
      "\n",
      "Dolan,  W.B.,  Pinkham,  J.,  Richardson,  S.D.,  2002.  MSR-MT:  The  Microsoft  research  machine \n",
      "translation system. In: Lecture Notes in Computer Science (Including Subseries Lecture Notes \n",
      "in Artificial Intelligence and Lecture Notes in Bioinformatics). Springer Verlag, pp. 237 239. \n",
      "\n",
      "Kalchbrenner, N., Blunsom, P., 2013. Recurrent Continous Translation Models. In: Proceedings \n",
      "of the 2013 Conference on Empirical Methods in Natural Language Processing. Association \n",
      "for Computational Linguistics, Seattle, USA, pp. 1700 1709. \n",
      "\n",
      "Koehn, P., 2010. Statistical Machine Translation. Cambridge University Press. \n",
      "\n",
      "Lopez, A., 2008. Statistical machine translation. ACM Comput. Surv. 40, 1 49. \n",
      "\n",
      "Marzouk, S., Hansen-Schirra, S., 2019. Evaluation of the impact of controlled language on neural \n",
      "\n",
      "machine translation compared to other MT architectures. Mach. Transl. 33, 179 203. \n",
      "\n",
      "Munk,  M.,  Munkova,  D.,  2018.  Detecting  errors  in  machine  translation  using  residuals  and \n",
      "\n",
      "metrics of automatic evaluation. J. Intell. Fuzzy Syst. 34, 3211 3223. \n",
      "\n",
      "System  for  Post-Editing  and  Automatic  Error \n",
      "\n",
      "Conference on Distance Learning in Applied Informatics, Sturovo, May 2 \n",
      "Kluwer, ISSN 2464-7489, Sturovo, pp. 571 579. \n",
      "\n",
      " 4, 2016. Wolters \n",
      "\n",
      "Munkova,  D.,  Munk,  M.,  2015.  Automatic  Evaluation  of  Machine  Translation  Through  the \n",
      "Residual  Analysis.  In:  Huang,  DS  and  Han,  K  (Ed.),  ADVANCED  INTELLIGENT  COMPUTING \n",
      "THEORIES AND APPLICATIONS, ICIC 2015, PT III, Lecture Notes in Artificial Intelligence. pp. \n",
      "481 490. \n",
      "\n",
      "DIVAI 2020 \n",
      "ISBN 978-80-7598-841-6 ISSN 2464-7470 (Print) ISSN 2464-7489 (On-line) \n",
      "\n",
      " The 13th international scientific conference on Distance Learning in Applied Informatics. \n",
      "\n",
      "507 \n",
      "\n",
      " \n",
      " \n",
      "\fNeural Machine Translation as a Novel Approach to Machine Translation \n",
      "\n",
      "Nitre, Nitra. \n",
      "\n",
      "-\n",
      "edited  MT  Output  for  Genealogically  Related  Languages.  In:  Innovation  in  Information \n",
      "Systems and Technologies to Support Learning Research. EMENA-ISTL 2019. Springer, Cham, \n",
      "pp. 416 425. \n",
      "\n",
      "Transaction/Sequence Model. Procedia Comput. Sci. 18, 1198 1207. \n",
      "\n",
      "-processing  Evaluation  for  Text  Mining: \n",
      "\n",
      "identification within comparable corpora, Advances in Intelligent Systems and Computing. \n",
      "\n",
      "-words removal on sequence patterns \n",
      "\n",
      "Munkova, D., Wrede, O., Absolon, J., 2019. Comparison of human, machine and Post-Editing \n",
      "Translation from Slovak into German by means of automatic Evaluation. ZEITSCHRIFT FUR \n",
      "SLAWISTIK 64, 231 261. \n",
      "\n",
      "chine translation for \n",
      "highly  inflected  and  small  languages.  In:  Lecture  Notes  in  Computer  Science  (Including \n",
      "Subseries  Lecture  Notes  in  Artificial  Intelligence  and  Lecture  Notes  in  Bioinformatics). \n",
      "Springer Verlag, pp. 445 456. \n",
      "\n",
      "s,  V.,  Miks,  T.,  2018b.  Integration  of  neural  machine  translation \n",
      "systems  for  formatting-rich  document  translation.  In:  Lecture  Notes  in  Computer  Science \n",
      "in \n",
      "in  Artificial \n",
      "(Including  Subseries  Lecture  Notes \n",
      "Bioinformatics). Springer Verlag, pp. 494 497.\n",
      "\n",
      "Intelligence  and  Lecture  Notes \n",
      "\n",
      "Stencl,  M.,  Stastny,  J.,  2010.  Neural  network  learning  algorithms  comparison  on  numerical \n",
      "prediction of real data. In: 16th International Conference on Soft Computing MENDEL 2010, \n",
      "Brno. Brno, pp. 280 285. \n",
      "\n",
      "Sutskever, I., Vinyals, O., Le, Q. V., 2014. Sequence to sequence learning with neural networks. \n",
      "\n",
      "Adv. Neural Inf. Process. Syst. 3104 3112. \n",
      "\n",
      "-A.,  2015.  LeBLEU:  N-gram-based  Translation  Evaluation  Score  for \n",
      "Morphologically Complex Languages. In: Proceedings of the Tenth Workshop on Statistical \n",
      "Machine Translation. Association for Computational Linguistics (ACL), pp. 411 416. \n",
      "\n",
      "Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., \n",
      "\n",
      "Macherey, K., Klingner, J., Shah, \n",
      "T.,  Kazawa,  H.,  Stevens,  K.,  Kurian,  G.,  Patil,  N.,  Wang,  W.,  Young,  C.,  Smith,  J.,  Riesa,  J., \n",
      "\n",
      "Translation System: Bridging the Gap between Human and Machine Translation. \n",
      "\n",
      "Xia, Y., 2019. Research on statistical machine translation model based on deep neural network. \n",
      "\n",
      "Computing 1 19. \n",
      "\n",
      "Yamamoto,  H.,  Isogai,  S.,  Sagisaka,  Y.,  2003.  Multi-class  composite  N-gram  language  model. \n",
      "\n",
      "Speech Commun. 41, 369 379. \n",
      "\n",
      "DIVAI 2020 \n",
      "ISBN 978-80-7598-841-6 ISSN 2464-7470 (Print) ISSN 2464-7489 (On-line) \n",
      "\n",
      " The 13th international scientific conference on Distance Learning in Applied Informatics. \n",
      "\n",
      "508 \n",
      "\n",
      "View publication stats\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "print(pdf_file_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
